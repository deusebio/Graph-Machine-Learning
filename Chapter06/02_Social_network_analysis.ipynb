{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link prediction on social network using DGL"
      ],
      "metadata": {
        "id": "G-iq6EXuNk18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqMpPrND8SeA"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y dgl\n",
        "!pip install  dgl==2.2.1 -f https://data.dgl.ai/wheels/torch-2.3/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the social network graph\n",
        "import pickle\n",
        "with open('test.gpickle', 'rb') as f:\n",
        "  Gnx = pickle.load(f)"
      ],
      "metadata": {
        "id": "XK8GxEU285LJ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "# convert the graph from networkx to dgl. We are now ready to start learning\n",
        "G = dgl.from_networkx(Gnx)"
      ],
      "metadata": {
        "id": "2BFI_Tus_9Rv"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we are implementing a GraphSAGE model to perform link prediction on a graph using the Deep Graph Library (DGL) and PyTorch. We start by setting up the computational device and initializing the node features as identity matrices. The graph's edges are then split into training and testing sets to evaluate the model's performance on unseen data. Negative edges are sampled to serve as negative examples during training. We define a GraphSAGE model with two layers that aggregate neighbor information and a dot-product-based edge predictor to compute edge scores. The model is trained using binary cross-entropy loss, optimized with the Adam optimizer. After training for a specified number of epochs, we evaluate the model's performance using common metrics on the test set."
      ],
      "metadata": {
        "id": "u88BLEDkJpJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import SAGEConv\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch import nn\n",
        "import itertools\n",
        "import dgl.function as fn\n",
        "\n",
        "# Set the computation device to GPU if available, otherwise CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming graph G is pre-defined and moving it to the computation device\n",
        "graph = G.to(device)"
      ],
      "metadata": {
        "id": "xO0n7SsKKDUP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the graph is loaded, we need to perform the following steps:\n",
        "- assign the fake features (i.e. the identity matrix)\n",
        "- splitting edges into training edges (90%) and test edges (10%)"
      ],
      "metadata": {
        "id": "xhgraf86KGMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning a unique identity feature to each node\n",
        "# This helps the model to have initial distinguishable features for each node\n",
        "node_features = torch.eye(graph.number_of_nodes()).to(device)\n",
        "graph.ndata['features'] = node_features\n",
        "\n",
        "# Splitting edges into training and test sets\n",
        "# This helps in evaluating the model performance on unseen data\n",
        "src_nodes, dst_nodes = graph.edges()\n",
        "edge_ids = np.arange(graph.number_of_edges())\n",
        "np.random.shuffle(edge_ids)\n",
        "\n",
        "# Define the number of test edges (10% of total edges)\n",
        "test_edge_count = int(0.1 * len(edge_ids))\n",
        "train_edge_count = len(edge_ids) - test_edge_count"
      ],
      "metadata": {
        "id": "T3lgwzSHD9Ka"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to find negative (i.e. non existent) edges. This because we may want to train the model whether an edge exists.. or not!\n",
        "We will be doing this by defining an adjacency matrix and randomly picking negative edges.\n",
        "\n",
        "Finally, we create a test graph for model evaluation."
      ],
      "metadata": {
        "id": "Jce-FNxeKqcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting edges into positive training and testing sets\n",
        "# Positive edges simulate the real edges in the graph\n",
        "test_pos_src, test_pos_dst = src_nodes[edge_ids[:test_edge_count]], dst_nodes[edge_ids[:test_edge_count]]\n",
        "train_pos_src, train_pos_dst = src_nodes[edge_ids[test_edge_count:]], dst_nodes[edge_ids[test_edge_count:]]\n",
        "\n",
        "# Creating an adjacency matrix and finding negative edges\n",
        "# Negative edges are non-existent edges in the graph used for negative sampling\n",
        "adj_matrix = sp.coo_matrix((np.ones(len(src_nodes)), (src_nodes.numpy(), dst_nodes.numpy())), shape=(graph.number_of_nodes(), graph.number_of_nodes()))\n",
        "neg_adj_matrix = 1 - adj_matrix.toarray() - np.eye(graph.number_of_nodes())\n",
        "neg_src, neg_dst = np.where(neg_adj_matrix != 0)\n",
        "neg_edge_ids = np.random.choice(len(neg_src), size=graph.number_of_edges(), replace=False)\n",
        "\n",
        "# Splitting negative edges into training and testing sets\n",
        "# These edges serve as negative samples during training and testing\n",
        "test_neg_src, test_neg_dst = neg_src[neg_edge_ids[:test_edge_count]], neg_dst[neg_edge_ids[:test_edge_count]]\n",
        "train_neg_src, train_neg_dst = neg_src[neg_edge_ids[test_edge_count:]], neg_dst[neg_edge_ids[test_edge_count:]]\n",
        "\n",
        "# Creating a training graph by removing test edges\n",
        "# This prevents the model from training on test data and helps evaluate its generalization capability\n",
        "train_graph = dgl.remove_edges(graph, edge_ids[:test_edge_count])"
      ],
      "metadata": {
        "id": "_1_fmldeKo86"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train the model.\n",
        "The next steps are the followings:-\n",
        "- create a GNN model (we choose a GraphSAGE model in this case)\n",
        "- attach an edge predictor (in this case we choose to compute the \"existence\" score for an edge by taking the dot product of the embeddings of the two end nodes\n",
        "- implement the train loop which computes the predictions, the loss value, and applies backpropagate to update the model weights."
      ],
      "metadata": {
        "id": "GPAANUqWLX5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the GraphSAGE model\n",
        "# This model consists of two GraphSAGE layers, each computes new node representations by averaging neighbor information\n",
        "# DGL provides dgl.nn.SAGEConv that conveniently creates a GraphSAGE layer\n",
        "class GraphSAGENetwork(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats):\n",
        "        super(GraphSAGENetwork, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, hidden_feats, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(hidden_feats, hidden_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h = self.conv1(g, features)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "# Defining the edge predictor using dot product\n",
        "# This predictor computes the score for an edge by taking the dot product of the embeddings of the two end nodes\n",
        "class DotProductPredictor(nn.Module):\n",
        "    def forward(self, graph, node_embeddings):\n",
        "        with graph.local_scope():\n",
        "            graph.ndata['h'] = node_embeddings\n",
        "            graph.apply_edges(dgl.function.u_dot_v('h', 'h', 'score'))\n",
        "            return graph.edata['score'][:, 0]\n",
        "\n",
        "# Initialize the GraphSAGE model and the predictor\n",
        "sage_model = GraphSAGENetwork(graph.ndata['features'].shape[1], 16).to(device)\n",
        "predictor = DotProductPredictor().to(device)\n",
        "\n",
        "# Function to compute the loss\n",
        "# This combines the positive and negative scores and uses binary cross-entropy loss to measure performance\n",
        "def compute_loss(pos_scores, neg_scores):\n",
        "    scores = torch.cat([pos_scores, neg_scores])\n",
        "    labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
        "\n",
        "# Optimizer setup\n",
        "# Using Adam optimizer to update model parameters based on the gradients computed during backpropagation\n",
        "optimizer = torch.optim.Adam(itertools.chain(sage_model.parameters(), predictor.parameters()), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "# The model is trained for a specified number of epochs\n",
        "for epoch in range(100):\n",
        "    sage_model.train()\n",
        "\n",
        "    # Compute node embeddings\n",
        "    node_embeddings = sage_model(train_graph, train_graph.ndata['features'])\n",
        "\n",
        "    # Compute scores for positive and negative edges\n",
        "    pos_scores = predictor(dgl.graph((train_pos_src, train_pos_dst), num_nodes=graph.number_of_nodes()).to(device), node_embeddings)\n",
        "    neg_scores = predictor(dgl.graph((train_neg_src, train_neg_dst), num_nodes=graph.number_of_nodes()).to(device), node_embeddings)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(pos_scores, neg_scores)\n",
        "\n",
        "    # Backward pass: compute gradients and update model parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV_OfWU3LWJx",
        "outputId": "7a68eda0-e744-4f74-8317-3dc06e351b2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7107084393501282\n",
            "Epoch 5, Loss: 0.6259969472885132\n",
            "Epoch 10, Loss: 0.496582567691803\n",
            "Epoch 15, Loss: 0.48763927817344666\n",
            "Epoch 20, Loss: 0.45504021644592285\n",
            "Epoch 25, Loss: 0.4441237151622772\n",
            "Epoch 30, Loss: 0.43200087547302246\n",
            "Epoch 35, Loss: 0.42003700137138367\n",
            "Epoch 40, Loss: 0.41148093342781067\n",
            "Epoch 45, Loss: 0.40247464179992676\n",
            "Epoch 50, Loss: 0.3950021266937256\n",
            "Epoch 55, Loss: 0.3874279856681824\n",
            "Epoch 60, Loss: 0.38146260380744934\n",
            "Epoch 65, Loss: 0.3763078451156616\n",
            "Epoch 70, Loss: 0.37132003903388977\n",
            "Epoch 75, Loss: 0.3667740821838379\n",
            "Epoch 80, Loss: 0.3623138666152954\n",
            "Epoch 85, Loss: 0.3579651117324829\n",
            "Epoch 90, Loss: 0.35385265946388245\n",
            "Epoch 95, Loss: 0.350058913230896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model by means of f1-score, precision and recall."
      ],
      "metadata": {
        "id": "CfpuLRMBMKq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize(scores):\n",
        "  return (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "# Define the score computation to evaluate model performance on classification tasks\n",
        "def compute_scores(positive_scores, negative_scores):\n",
        "    scores = torch.cat([positive_scores, negative_scores]).numpy()\n",
        "    labels = torch.cat([torch.ones(positive_scores.shape[0]), torch.zeros(negative_scores.shape[0])]).numpy()\n",
        "    return (f1_score(labels, scores),\n",
        "            precision_score(labels, scores),\n",
        "            recall_score(labels, scores))\n",
        "\n",
        "test_pos_graph = dgl.graph((test_pos_src, test_pos_dst), num_nodes=graph.number_of_nodes()).to(device)\n",
        "test_neg_graph = dgl.graph((test_neg_src, test_neg_dst), num_nodes=graph.number_of_nodes()).to(device)\n",
        "test_node_embeddings = sage_model(graph, graph.ndata['features'])\n",
        "\n",
        "# Evaluate model performance using proper metrics\n",
        "with torch.no_grad():\n",
        "    test_pos_scores = predictor(test_pos_graph, test_node_embeddings)\n",
        "    test_neg_scores = predictor(test_neg_graph, test_node_embeddings)\n",
        "\n",
        "    pos_test_scores = predictor(test_pos_graph, node_embeddings)\n",
        "    neg_test_scores = predictor(test_neg_graph, node_embeddings)\n",
        "\n",
        "    pos_test_scores = (normalize(pos_test_scores) > 0.5) * 1\n",
        "    neg_test_scores = (normalize(neg_test_scores) > 0.5) * 1\n",
        "\n",
        "    f1, prec, rec = compute_scores(pos_test_scores, neg_test_scores)\n",
        "    print(f'F1 Score: {f1}')\n",
        "    print(f'Precision: {prec}')\n",
        "    print(f'Recall: {rec}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkEwfrhPJUzW",
        "outputId": "89ce9634-98b0-433c-ecec-2c944de83a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.6530583954254029\n",
            "Precision: 0.8405745383174235\n",
            "Recall: 0.5339453700555367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQVSkOiHNCv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with large graphs\n",
        "In the previous example we have seen how to predict link using DGL. However, you may have noticed that we computed the probability of all edges at once during training, which, in case of large graphs, is not feasible.\n",
        "\n",
        "To overcome this issue, we can use some functionalities provided by graph machine learning libraries, including DGL. In the next example, instead of fitting the whole graph in memory, we will be iterating over the edges in minibatches.\n",
        "\n",
        "For readability we are not going to implement the validation and testing part, however it can be done as we have done above!"
      ],
      "metadata": {
        "id": "pSeHwaf0In_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DGL provides dgl.dataloading.EdgeDataLoader to iterate over edges for edge classification or link prediction tasks.\n",
        "# For link prediction, we also need to specify a negative sampler\n",
        "# builtin negative samplers ( non-existing edges) such as dgl.dataloading.negative_sampler.Uniform can be used for this purpose.\n",
        "\n",
        "# load 5 negative sample per each positive sample (existing edges)\n",
        "negative_sampler = dgl.dataloading.negative_sampler.Uniform(5)\n",
        "\n",
        "# define the edge loader\n",
        "sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n",
        "sampler = dgl.dataloading.as_edge_prediction_sampler(\n",
        "    sampler, negative_sampler=negative_sampler)\n",
        "\n",
        "dataloader = dgl.dataloading.DataLoader(\n",
        "    # The following arguments are specific to NodeDataLoader.\n",
        "    graph,                                      # The graph\n",
        "    torch.arange(graph.number_of_edges()),  # The edges to iterate over\n",
        "    sampler,                                # The neighbor sampler\n",
        "    device=device,                          # Put the MFGs on CPU or GPU\n",
        "    # The following arguments are inherited from PyTorch DataLoader.\n",
        "    batch_size=128,    # Batch size\n",
        "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
        "    drop_last=False,    # Whether to drop the last incomplete batch\n",
        "    num_workers=0       # Number of sampler processes\n",
        ")"
      ],
      "metadata": {
        "id": "H_fLo8j8InGq"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_nodes, pos_graph, neg_graph, mfgs = next(iter(dataloader))\n",
        "print('Number of input nodes:', len(input_nodes))\n",
        "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
        "print('Negative graph # nodes:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
        "\n",
        "print(mfgs)\n",
        "# Notice that the last element is a list of message flow graphs (MFGs) storing the computation dependencies for each GNN layer.\n",
        "# The MFGs are used to compute the GNN outputs of the nodes involved in positive/negative graph.\n",
        "# Check more on https://docs.dgl.ai/en/0.8.x/generated/dgl.dataloading.BlockSampler.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzvBz4sInOq",
        "outputId": "812c2a79-7f6c-4565-88ac-cdb55380b0fa"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of input nodes: 4039\n",
            "Positive graph # nodes: 808 # edges: 128\n",
            "Negative graph # nodes: 808 # edges: 640\n",
            "[Block(num_src_nodes=4039, num_dst_nodes=3975, num_edges=176281), Block(num_src_nodes=3975, num_dst_nodes=808, num_edges=48265)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import SAGEConv\n",
        "\n",
        "class GraphSAGENetwork(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats):\n",
        "        super(GraphSAGENetwork, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, hidden_feats, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(hidden_feats, hidden_feats, aggregator_type='mean')\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        h = self.conv1(g[0], features)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g[1], h)\n",
        "        return h\n",
        "\n",
        "# Defining the edge predictor using dot product\n",
        "# This predictor computes the score for an edge by taking the dot product of the embeddings of the two end nodes\n",
        "class DotProductPredictor(nn.Module):\n",
        "    def forward(self, graph, node_embeddings):\n",
        "        with graph.local_scope():\n",
        "            graph.ndata['h'] = node_embeddings\n",
        "            graph.apply_edges(dgl.function.u_dot_v('h', 'h', 'score'))\n",
        "            return graph.edata['score'][:, 0]\n",
        "\n",
        "# Initialize the GraphSAGE model and the predictor\n",
        "sage_model = GraphSAGENetwork(graph.ndata['features'].shape[1], 16).to(device)\n",
        "predictor = DotProductPredictor().to(device)\n",
        "\n",
        "# Optimizer setup\n",
        "# Using Adam optimizer to update model parameters based on the gradients computed during backpropagation\n",
        "optimizer = torch.optim.Adam(itertools.chain(sage_model.parameters(), predictor.parameters()), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "# The model is trained for a specified number of epochs\n",
        "for epoch in range(5):\n",
        "  total_loss = total_examples = 0\n",
        "  for (input_nodes, pos_graph, neg_graph, mfgs) in dataloader:\n",
        "    sage_model.train()\n",
        "\n",
        "    input_features = mfgs[0].srcdata['features']\n",
        "\n",
        "    # Compute node embeddings\n",
        "    node_embeddings = sage_model(mfgs, input_features)\n",
        "\n",
        "    # Compute scores for positive and negative edges\n",
        "    pos_scores = predictor(pos_graph, node_embeddings)\n",
        "    neg_scores = predictor(neg_graph, node_embeddings)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(pos_scores, neg_scores)\n",
        "\n",
        "    # Backward pass: compute gradients and update model parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += float(loss) * (len(pos_scores) + len(neg_scores))\n",
        "    total_examples += (len(pos_scores) + len(neg_scores))\n",
        "\n",
        "  print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg23gAE7MopC",
        "outputId": "3c132fb5-cef4-4361-cb96-3e13580994df"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py:1149: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
            "  dgl_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.7393\n",
            "Epoch: 001, Loss: 0.6965\n",
            "Epoch: 002, Loss: 0.6913\n",
            "Epoch: 003, Loss: 0.6819\n",
            "Epoch: 004, Loss: 0.6640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72eyir3cInRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link prediction on social network using PyG\n",
        "We will now replicate the example using another popular library for graph machine learning: Pytorch Geometric"
      ],
      "metadata": {
        "id": "5cPGcLx3NtA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric\n",
        "\n",
        "# Optional dependencies:\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cpu.html"
      ],
      "metadata": {
        "id": "_Smng0hRNtbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f31f79-a12c-49b2-9212-b0a5a0d901b6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.6.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cpu.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/pyg_lib-0.4.0%2Bpt23cpu-cp310-cp310-linux_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_scatter-2.1.2%2Bpt23cpu-cp310-cp310-linux_x86_64.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.7/510.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_sparse-0.6.18%2Bpt23cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_cluster-1.6.3%2Bpt23cpu-cp310-cp310-linux_x86_64.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt23cpu-cp310-cp310-linux_x86_64.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.8/215.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.25.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt23cpu torch_cluster-1.6.3+pt23cpu torch_scatter-2.1.2+pt23cpu torch_sparse-0.6.18+pt23cpu torch_spline_conv-1.2.2+pt23cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils.convert import from_networkx\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch_geometric.nn import SAGEConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Convert the graph into PyTorch geometric\n",
        "G = from_networkx(Gnx)"
      ],
      "metadata": {
        "id": "UuygURxWN-jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9e85a6-97b1-4569-cafe-d74115ef13a6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  data_dict[key] = torch.as_tensor(value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's add fake features\n",
        "G.x = torch.eye(G.num_nodes)"
      ],
      "metadata": {
        "id": "AGEAdLxMO8-w"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we first split the set of edges into training (80%), validation (10%),\n",
        "# and testing edges (10%). We also generate fixed negative (non existing)\n",
        "# edges for evaluation with a ratio of 2:1.\n",
        "# We can leverage the `RandomLinkSplit()` transform to perform all the steps:\n",
        "transform = T.RandomLinkSplit(\n",
        "    num_val=0.1,\n",
        "    num_test=0.1,\n",
        "    disjoint_train_ratio=0.3,\n",
        "    neg_sampling_ratio=2.0,\n",
        "    add_negative_train_samples=False\n",
        ")\n",
        "train_data, val_data, test_data = transform(G)"
      ],
      "metadata": {
        "id": "pBliMBhdOr1C"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to what we have done above, we will be using a mini-batch loader: our graph is quite small, so it is perfectly fine to load it in memory while training. However, for larger graphs, since computing the probability of all edges is usually not feasible, a mini-batch loader is required to load parts of the graph step by step.\n",
        "\n",
        "PyG makes use of the loader.LinkNeighborLoader to sample multiple hops from both ends of a link and creates a subgraph from it."
      ],
      "metadata": {
        "id": "stNh0Op1P9nG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define seed edges:\n",
        "edge_label_index = train_data.edge_label_index\n",
        "edge_label = train_data.edge_label\n",
        "train_loader = LinkNeighborLoader(\n",
        "    data=train_data,\n",
        "    num_neighbors=[20, 20],\n",
        "    neg_sampling_ratio=2.0,\n",
        "    edge_label_index=edge_label_index,\n",
        "    edge_label=edge_label,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "wy3Gvd0bPm5f"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the GraphSAGE model\n",
        "# This model consists of two GraphSAGE layers, each computes new node representations by averaging neighbor information\n",
        "class GraphSAGENetwork(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats):\n",
        "        super(GraphSAGENetwork, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, hidden_feats)\n",
        "        self.conv2 = SAGEConv(hidden_feats, hidden_feats)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(h, edge_index)\n",
        "        return h\n",
        "\n",
        "# Defining the edge predictor using dot product\n",
        "# This predictor computes the score for an edge by taking the dot product of the embeddings of the two end nodes\n",
        "class DotProductPredictor(nn.Module):\n",
        "    def forward(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        return (z[src] * z[dst]).sum(dim=-1)\n",
        "\n",
        "# Initialize the GraphSAGE model and the predictor\n",
        "sage_model = GraphSAGENetwork(G.num_features, 16).to(device)\n",
        "predictor = DotProductPredictor().to(device)"
      ],
      "metadata": {
        "id": "zk2gPxT0QGhq"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute the loss\n",
        "# This combines the positive and negative scores and uses binary cross-entropy loss to measure performance\n",
        "def compute_loss(pred, ground_truth):\n",
        "    loss = F.binary_cross_entropy_with_logits(pred, ground_truth)\n",
        "    return loss\n",
        "\n",
        "# Function to compute the prediction score\n",
        "def compute_scores(labels, scores):\n",
        "    return (f1_score(labels, scores),\n",
        "            precision_score(labels, scores),\n",
        "            recall_score(labels, scores))"
      ],
      "metadata": {
        "id": "ZDjttEQvQnn3"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Optimizer setup\n",
        "# Using Adam optimizer to update model parameters based on the gradients computed during backpropagation\n",
        "optimizer = torch.optim.Adam(itertools.chain(sage_model.parameters(), predictor.parameters()), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "# The model is trained for a specified number of epochs\n",
        "for epoch in range(1):\n",
        "    sage_model.train()\n",
        "    total_loss = total_examples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      batch.to(device)\n",
        "\n",
        "      # Compute node embeddings\n",
        "      node_embeddings = sage_model(batch.x, batch.edge_index)\n",
        "      scores = predictor(node_embeddings, batch.edge_label_index)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = compute_loss(scores, batch.edge_label)\n",
        "\n",
        "      # Backward pass: compute gradients and update model parameters\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += float(loss) * scores.numel()\n",
        "      total_examples += scores.numel()\n",
        "\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acSDECu0Q_Ja",
        "outputId": "68eceb76-9708-42c4-be1d-8454df84e356"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 331/331 [05:48<00:00,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.6045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the model. For doing this we will be creating a proper linkneighborloader"
      ],
      "metadata": {
        "id": "_bc8WNOFVd_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the validation seed edges:\n",
        "edge_label_index = val_data.edge_label_index\n",
        "edge_label = val_data.edge_label\n",
        "val_loader = LinkNeighborLoader(\n",
        "    data=val_data,\n",
        "    num_neighbors=[20, 20],\n",
        "    edge_label_index=edge_label_index,\n",
        "    edge_label=edge_label,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        ")\n",
        "sampled_data = next(iter(val_loader))\n",
        "sampled_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20mj7rUxQpD0",
        "outputId": "635a3bd2-983c-46e9-b8af-7701fe312e09"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 36312], features=[3256, 1283], num_nodes=3256, x=[3256, 4039], edge_label=[128], edge_label_index=[2, 128], n_id=[3256], e_id=[36312], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[128])"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "ground_truths = []\n",
        "\n",
        "for batch in tqdm(val_loader):\n",
        "    with torch.no_grad():\n",
        "        batch.to(device)\n",
        "\n",
        "        # compute predictions\n",
        "        node_embeddings = sage_model(batch.x, batch.edge_index)\n",
        "        scores = predictor(node_embeddings, batch.edge_label_index)\n",
        "\n",
        "        preds.append(scores)\n",
        "        ground_truths.append(batch.edge_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Oz9AzOrVs2i",
        "outputId": "3559ce46-8876-4f38-da29-e883104390c7"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 414/414 [05:47<00:00,  1.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(scores):\n",
        "  return (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
        "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
        "\n",
        "pred = normalize(pred) > 0.5\n",
        "ground_truth = normalize(ground_truth) > 0.5\n",
        "\n",
        "f1, prec, rec = compute_scores(ground_truth, pred)\n",
        "\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Precision: {prec}')\n",
        "print(f'Recall: {rec}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaV6LzVpZXaW",
        "outputId": "44b93179-7a0c-4ef1-f918-e2b6a93d8735"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.8262437773869497\n",
            "Precision: 0.8715804037481919\n",
            "Recall: 0.7853904567607389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k072d2xfahBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}